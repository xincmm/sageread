// src/llamacpp-client.ts
import { invoke } from "@tauri-apps/api/core";

// ç±»å‹å®šä¹‰
export interface SessionInfo {
  pid: number;
  port: number;
  model_id: string;
  model_path: string;
  api_key: string;
}

export interface DeviceInfo {
  id: string;
  name: string;
  memory: number;
}

export interface GgufMetadata {
  version: number;
  tensor_count: number;
  metadata: Record<string, string>;
}

// llamacpp å®¢æˆ·ç«¯ç±»
export class LlamacppClient {
  // åŠ è½½æ¨¡å‹
  async loadModel(backendPath: string, libraryPath?: string, args: string[] = []): Promise<SessionInfo> {
    return await invoke("plugin:llamacpp|load_llama_model", {
      backendPath,
      libraryPath,
      args,
    });
  }

  // å¸è½½æ¨¡å‹
  async unloadModel(pid: number): Promise<void> {
    return await invoke("plugin:llamacpp|unload_llama_model", { pid });
  }

  // è·å–è®¾å¤‡ä¿¡æ¯
  async getDevices(backendPath: string, libraryPath?: string): Promise<DeviceInfo[]> {
    return await invoke("plugin:llamacpp|get_devices", {
      backendPath,
      libraryPath,
    });
  }

  // æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿è¡Œ
  async isProcessRunning(pid: number): Promise<boolean> {
    return await invoke("plugin:llamacpp|is_process_running", { pid });
  }

  // è·å–éšæœºç«¯å£
  async getRandomPort(): Promise<number> {
    return await invoke("plugin:llamacpp|get_random_port");
  }

  // æ ¹æ®æ¨¡å‹IDæŸ¥æ‰¾ä¼šè¯
  async findSessionByModel(modelId: string): Promise<SessionInfo | null> {
    return await invoke("plugin:llamacpp|find_session_by_model", { modelId });
  }

  // è·å–æ‰€æœ‰å·²åŠ è½½çš„æ¨¡å‹
  async getLoadedModels(): Promise<string[]> {
    return await invoke("plugin:llamacpp|get_loaded_models");
  }

  // è·å–æ‰€æœ‰æ´»è·ƒä¼šè¯
  async getAllSessions(): Promise<SessionInfo[]> {
    return await invoke("plugin:llamacpp|get_all_sessions");
  }

  // è¯»å– GGUF å…ƒæ•°æ®
  async readGgufMetadata(path: string): Promise<GgufMetadata> {
    return await invoke("plugin:llamacpp|read_gguf_metadata", { path });
  }

  // æ¸…ç†æ‰€æœ‰è¿›ç¨‹
  async cleanupProcesses(): Promise<void> {
    return await invoke("plugin:llamacpp|cleanup_llama_processes");
  }

  // ç”Ÿæˆ API å¯†é’¥
  async generateApiKey(modelId: string, apiSecret: string): Promise<string> {
    return await invoke("plugin:llamacpp|generate_api_key", { modelId, apiSecret });
  }
}

// Embedding æœåŠ¡å™¨ç®¡ç†ç±» - ä½¿ç”¨ tauri-plugin-llamacpp çš„è‡ªåŠ¨åç«¯ç®¡ç†
export class LlamaServerManager {
  private client: LlamacppClient;

  constructor() {
    this.client = new LlamacppClient();
  }

  // è·å–åº”ç”¨æ•°æ®ç›®å½•ä¸­çš„åç«¯è·¯å¾„ï¼Œå¹¶ç¡®ä¿åç«¯å·²ä¸‹è½½
  private async ensureBackendReady(): Promise<string> {
    try {
      // 1. ç¡®ä¿ llamacpp ç›®å½•ç»“æ„å­˜åœ¨
      await invoke<string>("ensure_llamacpp_directories");
      console.log("âœ… LlamaCpp ç›®å½•ç»“æ„å·²åˆ›å»º");

      // 2. ä¸‹è½½ llama-serverï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
      console.log("ğŸ“¥ æ£€æŸ¥å¹¶ä¸‹è½½ llama-server...");
      const downloadResult = await invoke<string>("download_llama_server");
      console.log("ğŸ“¥", downloadResult);

      // 3. è·å–åº”ç”¨æ•°æ®ç›®å½•åç«¯è·¯å¾„
      const backendPath = await invoke<string>("get_llamacpp_backend_path");
      console.log("ğŸ“‚ åç«¯è·¯å¾„:", backendPath);
      return backendPath;
    } catch (error) {
      console.log("è‡ªåŠ¨åç«¯ç®¡ç†å¤±è´¥ï¼Œå°è¯•ç³»ç»Ÿè·¯å¾„:", error);

      // å¦‚æœå¤±è´¥ï¼Œå°è¯•ç³»ç»Ÿå®‰è£…çš„è·¯å¾„
      const systemPaths = [
        "/opt/homebrew/bin/llama-server", // Homebrew macOS
        "/usr/local/bin/llama-server", // æ‰‹åŠ¨å®‰è£…
        "/usr/bin/llama-server", // ç³»ç»ŸåŒ…ç®¡ç†å™¨
        "llama-server", // PATH ä¸­
      ];

      console.log("ğŸ”„ ä½¿ç”¨ç³»ç»Ÿè·¯å¾„:", systemPaths[0]);
      return systemPaths[0]; // è¿”å›ç¬¬ä¸€ä¸ªä½œä¸ºå°è¯•
    }
  }

  // å¯åŠ¨ Embedding æœåŠ¡å™¨
  async startEmbeddingServer(modelPath: string): Promise<SessionInfo> {
    try {
      console.log("æ­£åœ¨å¯åŠ¨ Embedding æœåŠ¡å™¨...");
      console.log("æ¨¡å‹è·¯å¾„:", modelPath);

      // ç¡®ä¿åç«¯å·²å‡†å¤‡å°±ç»ª
      const backendPath = await this.ensureBackendReady();
      console.log("âœ… åç«¯å·²å‡†å¤‡å°±ç»ª:", backendPath);

      // è·å–éšæœºç«¯å£
      const serverPort = await this.client.getRandomPort();

      // æ„å»º embedding å‚æ•° - ä½¿ç”¨æœ€åŸºæœ¬çš„å‚æ•°ç¡®ä¿å…¼å®¹æ€§
      const embeddingArgs = [
        "--port",
        serverPort.toString(), // æœåŠ¡ç«¯å£
        "--host",
        "0.0.0.0", // ç»‘å®šåˆ°æœ¬åœ°
        "-m",
        modelPath, // æ¨¡å‹è·¯å¾„
        "--embedding", // å¯ç”¨ embedding æ¨¡å¼
        "-c",
        "1024", // ä¸Šä¸‹æ–‡é•¿åº¦
        "--threads",
        "4", // CPU çº¿ç¨‹æ•°
      ];

      console.log("å¯åŠ¨å‚æ•°:", embeddingArgs);

      // è°ƒç”¨æ’ä»¶çš„ load_llama_model å‘½ä»¤
      const session = await this.client.loadModel(backendPath, undefined, embeddingArgs);

      console.log("ğŸ‰ Embedding æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ:", {
        pid: session.pid,
        port: session.port,
        model_id: session.model_id,
        model_path: session.model_path,
        api_endpoint: `http://127.0.0.1:${session.port}/v1/embeddings`,
      });

      return session;
    } catch (error) {
      console.error("âŒ å¯åŠ¨ Embedding æœåŠ¡å™¨å¤±è´¥:", error);

      // æä¾›è§£å†³æ–¹æ¡ˆ
      if (error instanceof Error && error.message.includes("Binary not found")) {
        console.log("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:");
        console.log("1. å®‰è£… llama.cpp:");
        console.log("   macOS: brew install llama.cpp");
        console.log("   Ubuntu: sudo apt install llama.cpp");
        console.log("   æˆ–ä»æºç ç¼–è¯‘: https://github.com/ggerganov/llama.cpp");
        console.log("");
        console.log("2. æˆ–è€…ç­‰å¾…æ’ä»¶è‡ªåŠ¨ä¸‹è½½åŠŸèƒ½å®ç°");
        throw new Error("llama-server æœªæ‰¾åˆ°ã€‚è¯·å…ˆå®‰è£… llama.cppï¼Œæˆ–ç¡®ä¿ llama-server åœ¨ PATH ä¸­ã€‚");
      }

      throw error;
    }
  }

  // åœæ­¢æœåŠ¡å™¨
  async stopServer(session: SessionInfo): Promise<void> {
    try {
      console.log(`æ­£åœ¨åœæ­¢æœåŠ¡å™¨ PID: ${session.pid}`);
      await this.client.unloadModel(session.pid);
      console.log("æœåŠ¡å™¨å·²æˆåŠŸåœæ­¢");
    } catch (error) {
      console.error("åœæ­¢æœåŠ¡å™¨å¤±è´¥:", error);
      throw error;
    }
  }

  // æµ‹è¯• embedding åŠŸèƒ½
  async testEmbedding(session: SessionInfo, text: string): Promise<any> {
    try {
      const response = await fetch(`http://127.0.0.1:${session.port}/v1/embeddings`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${session.api_key}`,
        },
        body: JSON.stringify({
          input: [text],
          model: session.model_id,
          encoding_format: "float",
        }),
      });

      if (response.ok) {
        const result = await response.json();
        console.log("Embedding æµ‹è¯•æˆåŠŸ:", {
          text: text,
          embedding_length: result.data?.[0]?.embedding?.length || 0,
          first_few_values: result.data?.[0]?.embedding?.slice(0, 5) || [],
        });
        return result;
      }
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    } catch (error) {
      console.error("Embedding æµ‹è¯•å¤±è´¥:", error);
      throw error;
    }
  }
}

// é€šç”¨ä½¿ç”¨ç¤ºä¾‹
export async function exampleUsage() {
  const client = new LlamacppClient();

  try {
    // 1. è·å–å¯ç”¨è®¾å¤‡ (ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„è·¯å¾„)
    const devices = await client.getDevices("/usr/local/bin/llama-server");
    console.log("å¯ç”¨è®¾å¤‡:", devices);

    // 2. åŠ è½½æ¨¡å‹
    const session = await client.loadModel("/usr/local/bin/llama-server", undefined, [
      "-m",
      "/path/to/model.gguf",
      "--port",
      "8080",
      "--host",
      "127.0.0.1",
      "-c",
      "2048",
      "-ngl",
      "32",
    ]);
    console.log("æ¨¡å‹å·²åŠ è½½:", session);

    // 3. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
    const isRunning = await client.isProcessRunning(session.pid);
    console.log("è¿›ç¨‹è¿è¡ŒçŠ¶æ€:", isRunning);

    // 4. è·å–æ‰€æœ‰ä¼šè¯
    const allSessions = await client.getAllSessions();
    console.log("æ‰€æœ‰ä¼šè¯:", allSessions);

    // 5. å¸è½½æ¨¡å‹
    await client.unloadModel(session.pid);
    console.log("æ¨¡å‹å·²å¸è½½");
  } catch (error) {
    console.error("æ“ä½œå¤±è´¥:", error);
  }
}
